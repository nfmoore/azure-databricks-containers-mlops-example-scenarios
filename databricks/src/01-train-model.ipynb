{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c25ea4c-a3f8-4d7a-a605-d883d4278d98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Train machine learning model\n",
    "\n",
    "This notebook outlines a workflow for training a machine learning model with the goal of identifying optimal hyperparameters. The `UCI Credit Card Client Default` [dataset](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients) will be used to develop a machine learning model to predict the liklihood of credit default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce9e41b8-c45b-44b5-8081-739f85051e44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Import dependencies, define notebook parameters and constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8056838a-a538-4f17-9ee6-ef5d0e71dcf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Tuple, Union\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, fmin, hp, tpe\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c977f935-1edf-497c-91c1-7d19c0cd2b06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define notebook parameters\n",
    "dbutils.widgets.text(\"experiment_name\", \"/online-inference-containers-examples\")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"curated_dataset_table\", \"hive_metastore.default.credit_default_uci_curated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda4b332-f396-479b-aa52-bbd182b08c17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define target column\n",
    "TARGET = [\"default_payment_next_month\"]\n",
    "\n",
    "# define categorical feature columns\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"sex\",\n",
    "    \"education\",\n",
    "    \"marriage\",\n",
    "    \"repayment_status_1\",\n",
    "    \"repayment_status_2\",\n",
    "    \"repayment_status_3\",\n",
    "    \"repayment_status_4\",\n",
    "    \"repayment_status_5\",\n",
    "    \"repayment_status_6\",\n",
    "]\n",
    "\n",
    "# define numeric feature columns\n",
    "NUMERIC_FEATURES = [\n",
    "    \"credit_limit\",\n",
    "    \"age\",\n",
    "    \"bill_amount_1\",\n",
    "    \"bill_amount_2\",\n",
    "    \"bill_amount_3\",\n",
    "    \"bill_amount_4\",\n",
    "    \"bill_amount_5\",\n",
    "    \"bill_amount_6\",\n",
    "    \"payment_amount_1\",\n",
    "    \"payment_amount_2\",\n",
    "    \"payment_amount_3\",\n",
    "    \"payment_amount_4\",\n",
    "    \"payment_amount_5\",\n",
    "    \"payment_amount_6\",\n",
    "]\n",
    "\n",
    "# define all features\n",
    "FEATURES = CATEGORICAL_FEATURES + NUMERIC_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80db3cfe-6b32-44ce-a93d-c95ed42592f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define functions to build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f6cd74-7251-4d5b-b4ab-9547e740793a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_classifer_pipeline(params: Dict[str, Union[str, int]]) -> Pipeline:\n",
    "    \"\"\"Create sklearn pipeline to apply transforms and a final estimator\"\"\"\n",
    "    # categorical features transformations\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\n",
    "                \"ohe\",\n",
    "                OneHotEncoder(\n",
    "                    handle_unknown=\"ignore\",\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # numeric features transformations\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "    )\n",
    "\n",
    "    # preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"categorical\", categorical_transformer, CATEGORICAL_FEATURES),\n",
    "            (\"numeric\", numeric_transformer, NUMERIC_FEATURES),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # model training pipeline\n",
    "    classifer_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(**params, n_jobs=-1)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return classifer_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db1b591-3790-4233-81b7-9588754f58e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define objective function\n",
    "def hyperparameter_tuning(params):\n",
    "    mlflow.sklearn.autolog(silent=True)\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # read and process curated data\n",
    "        df = spark.read.table(dbutils.widgets.get(\"curated_dataset_table\")).toPandas()\n",
    "\n",
    "        # split into train and test datasets\n",
    "        df_train, df_test = train_test_split(\n",
    "            df[CATEGORICAL_FEATURES + NUMERIC_FEATURES + TARGET],\n",
    "            test_size=0.20,\n",
    "            random_state=2024,\n",
    "        )\n",
    "\n",
    "        # seperate features and target variables\n",
    "        x_train, y_train = (\n",
    "            df_train[CATEGORICAL_FEATURES + NUMERIC_FEATURES],\n",
    "            df_train[TARGET],\n",
    "        )\n",
    "        x_test, y_test = (\n",
    "            df_test[CATEGORICAL_FEATURES + NUMERIC_FEATURES],\n",
    "            df_test[TARGET],\n",
    "        )\n",
    "\n",
    "        # train model\n",
    "        estimator = make_classifer_pipeline(params)\n",
    "        estimator.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "        # train and model\n",
    "        estimator = make_classifer_pipeline(params)\n",
    "        estimator = estimator.fit(x_train, y_train.values.ravel())\n",
    "        y_predict_proba = estimator.predict_proba(x_test)\n",
    "\n",
    "        # train model\n",
    "        estimator = make_classifer_pipeline(params)\n",
    "        estimator.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "        # calculate evaluation metrics\n",
    "        y_pred = estimator.predict(x_test)\n",
    "        validation_accuracy_score = accuracy_score(y_test.values.ravel(), y_pred)\n",
    "        validation_roc_auc_score = roc_auc_score(y_test.values.ravel(), y_pred)\n",
    "        validation_f1_score = f1_score(y_test.values.ravel(), y_pred)\n",
    "        validation_precision_score = precision_score(y_test.values.ravel(), y_pred)\n",
    "        validation_recall_score = recall_score(y_test.values.ravel(), y_pred)\n",
    "\n",
    "        # log evaluation metrics\n",
    "        mlflow.log_metric(\"validation_accuracy_score\", validation_accuracy_score)\n",
    "        mlflow.log_metric(\"validation_roc_auc_score\", validation_roc_auc_score)\n",
    "        mlflow.log_metric(\"validation_f1_score\", validation_f1_score)\n",
    "        mlflow.log_metric(\"validation_precision_score\", validation_precision_score)\n",
    "        mlflow.log_metric(\"validation_recall_score\", validation_recall_score)\n",
    "\n",
    "        # log model\n",
    "        signature = infer_signature(x_train, y_pred)\n",
    "        mlflow.sklearn.log_model(\n",
    "            estimator,\n",
    "            \"model\",\n",
    "            signature=signature,\n",
    "            input_example=x_test.iloc[0].to_dict(),\n",
    "        )\n",
    "\n",
    "        return {\"loss\": -validation_roc_auc_score, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8bb8fb-9767-411b-acc4-b5ecf273f445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # set mlflow tracking uri\n",
    "    mlflow_client = mlflow.tracking.MlflowClient(tracking_uri=\"databricks\")\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "    # start model training run\n",
    "    mlflow.set_experiment(dbutils.widgets.get(\"experiment_name\"))\n",
    "    with mlflow.start_run(run_name=\"credit-default-uci-train\") as run:\n",
    "        # define search space\n",
    "        search_space = {\n",
    "            \"n_estimators\": hp.choice(\"n_estimators\", range(100, 1000)),\n",
    "            \"max_depth\": hp.choice(\"max_depth\", range(1, 25)),\n",
    "            \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        }\n",
    "\n",
    "        # hyperparameter tuning\n",
    "        best_params = fmin(\n",
    "            fn=hyperparameter_tuning,\n",
    "            space=search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10,\n",
    "        )\n",
    "\n",
    "        # end run\n",
    "        mlflow.end_run()\n",
    "\n",
    "        return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71bc4da5-d840-487f-bf52-03797c7b0164",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Train the machine learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d2edb4-363f-483f-813c-fcc0375272c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "run = train_model()\n",
    "\n",
    "# retreive model from best run\n",
    "best_run = mlflow.search_runs(\n",
    "    filter_string=f\"tags.mlflow.parentRunId='{run.info.run_id}'\",\n",
    "    order_by=[\"metrics.validation_roc_auc_score DESC\"],\n",
    ").iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75a1e085-ec60-4958-9ec0-013ab9409642",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Return notebook outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78018db8-cd0e-45ae-8a7a-3d68c0946fa9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set best run id for task values\n",
    "dbutils.jobs.taskValues.set(key=\"best_run_id\", value=best_run.run_id)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-train-model",
   "widgets": {
    "curated_dataset_table": {
     "currentValue": "hive_metastore.default.credit_default_uci_curated",
     "nuid": "6a1769f3-7d95-4fc3-a19f-fe27769f2a7a",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "hive_metastore.default.credit_default_uci_curated",
      "label": null,
      "name": "curated_dataset_table",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "experiment_name": {
     "currentValue": "/online-inference-containers-examples",
     "nuid": "3576f59b-6263-44f1-b065-b050bc08a208",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "/online-inference-containers",
      "label": null,
      "name": "experiment_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
