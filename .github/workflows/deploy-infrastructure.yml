name: Deploy Infrastructure

on:
  workflow_dispatch:

env:
  AZ_CLI_VERSION: 2.59.0
  DATABRICKS_CLUSTER_NAME: default
  DEPLOYMENT_NAME: validate-${{ github.run_id }}
  DEPLOYMENT_LOCATION: ${{ vars.DEPLOYMENT_LOCATION }}
  DEPLOYMENT_RESOURCE_GROUP_NAME: ${{ vars.DEPLOYMENT_RESOURCE_GROUP_NAME }}
  DEPLOYMENT_DATARBICKS_MANAGED_RESOURCE_GROUP_NAME: ${{ vars.DEPLOYMENT_DATARBICKS_MANAGED_RESOURCE_GROUP_NAME }}
  TEMPLATE_FILE: infrastructure/main.biceps

permissions:
  id-token: write
  contents: read

jobs:
  build:
    name: Bicep Build
    runs-on: ubuntu-latest
    steps:
      # Checkout the repository to the GitHub Actions runner
      - name: Checkout repo
        uses: actions/checkout@v3

      # Authenticate to Az CLI using OIDC
      - name: "Azure CLI login"
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      # Checks that all Bicep configuration files adhere to a canonical format
      - name: Bicep lint
        uses: azure/cli@v2
        with:
          azcliversion: ${{ env.AZ_CLI_VERSION }}
          inlineScript: az bicep build --file ${TEMPLATE_FILE}

      # Validate whether the template is valid at subscription scope
      - name: Bicep validate
        uses: azure/cli@v2
        with:
          azcliversion: ${{ env.AZ_CLI_VERSION }}
          inlineScript: |
            az deployment sub validate \
              --name validate-${DEPLOYMENT_NAME} \
              --template-file ${TEMPLATE_FILE} \
              --location ${DEPLOYMENT_LOCATION} \
              --parameters resourceGroupName=${DEPLOYMENT_RESOURCE_GROUP_NAME} \
              --parameters mrgDatabricksName=${DEPLOYMENT_DATARBICKS_MANAGED_RESOURCE_GROUP_NAME} \
              --parameters location=${DEPLOYMENT_LOCATION}

  deploy:
    name: Bicep Deploy
    runs-on: ubuntu-latest
    needs: [build]
    steps:
      # Checkout the repository to the GitHub Actions runner
      - name: Checkout repo
        uses: actions/checkout@v3

      # Authenticate to Az CLI using OIDC
      - name: "Azure CLI login"
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

        # Deploy template to subscription
      - name: "Bicep deployment"
        uses: azure/cli@v2
        with:
          azcliversion: ${{ env.AZ_CLI_VERSION }}
          inlineScript: |
            az deployment sub create \
              --name validate-${DEPLOYMENT_NAME} \
              --template-file ${TEMPLATE_FILE} \
              --location ${DEPLOYMENT_LOCATION} \
              --parameters resourceGroupName=${DEPLOYMENT_RESOURCE_GROUP_NAME} \
              --parameters mrgDatabricksName=${DEPLOYMENT_DATARBICKS_MANAGED_RESOURCE_GROUP_NAME} \
              --parameters location=${DEPLOYMENT_LOCATION}
              > artifacts/deployment-output.json

      # Upload output from deployment
      - uses: actions/upload-artifact@v4
        with:
          name: artifacts
          path: artifacts

  setup:
    name: Databricks Setup
    runs-on: ubuntu-latest
    needs: [deploy]
    steps:
      # Checkout the repository to the GitHub Actions runner
      - name: Checkout repo
        uses: actions/checkout@v3

      # Download output from deployment
      - uses: actions/download-artifact@v4
        with:
          name: artifacts

      # Authenticate to Az CLI using OIDC
      - name: "Azure CLI login"
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      # Set Databricks host and token environment variables
      - name: Set Databricks environment variables
        uses: azure/cli@v2
        with:
          azcliversion: ${{ env.AZ_CLI_VERSION }}
          inlineScript: |
            echo "DATABRICKS_HOST=$(jq .properties.outputs.databricksHostname.value \
              deployment-output.json -r)" >> $GITHUB_ENV
              
            echo "DATABRICKS_TOKEN=$(az account get-access-token \
              --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d | jq .accessToken -r)" >> $GITHUB_ENV

      # Create Databricks cluster
      - name: Create Databricks cluster
        run: |
          curl -X POST ${DATABRICKS_HOST}/api/2.0/clusters/create \
            -H Authorization: Bearer ${DATABRICKS_TOKEN} \
            -d '{
                "num_workers": 0,
                "cluster_name": "${DATABRICKS_CLUSTER_NAME}",
                "spark_version": "14.3.x-cpu-ml-scala2.12",
                "spark_conf": {
                    "spark.master": "local[*, 4]",
                    "spark.databricks.cluster.profile": "singleNode"
                },
                "azure_attributes": {
                    "first_on_demand": 1,
                    "availability": "ON_DEMAND_AZURE",
                    "spot_bid_max_price": -1
                },
                "node_type_id": "Standard_D4ads_v5",
                "driver_node_type_id": "Standard_D4ads_v5",
                "autotermination_minutes": 60,
                "enable_elastic_disk": true,
                "enable_local_disk_encryption": false,
                "runtime_engine": "STANDARD"
            }' > artifacts/cluster-output.json

      # Set Databricks cluster id environment variable
      - name: Set Databricks environment variables
        run: |
          echo "DATABRICKS_CLUSTER_ID=$(jq .cluster_id artifacts/cluster-output.json -r)" >> $GITHUB_ENV

      # Upload files to DBFS
      - name: upload-dbfs-temp
        uses: databricks/upload-dbfs-temp@v0
        with:
          local-path: databricks/data/curated.csv
          dbfs-temp-dir: dbfs:/FileStore/tables/credit-card-default-uci-curated/01.csv
          databricks-host: ${{ env.DATABRICKS_HOST }}
          databricks-token: ${{ env.DATABRICKS_TOKEN }}

      # Trigger notebook to create external tables
      - name: Create external tables
        uses: databricks/run-notebook@v0
        with:
          databricks-host: ${{ env.DATABRICKS_HOST }}
          databricks-token: ${{ env.DATABRICKS_TOKEN }}
          local-notebook-path: databricks/src/00-create-external-table.ipynb
          existing-cluster-id: ${{ env.DATABRICKS_CLUSTER_ID }}
